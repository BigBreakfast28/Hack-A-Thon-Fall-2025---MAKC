{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70af7390-a574-4e75-b387-7d5e6e35a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee73773-f39e-4760-a514-0c623b2f9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SageMaker session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-2\n",
      "Role: arn:aws:iam::717494507037:role/service-role/AmazonSageMaker-ExecutionRole-20251115T112643\n",
      "Bucket: sagemaker-us-east-2-717494507037\n",
      "\n",
      "Defining pipeline parameters...\n",
      "Creating processing scripts...\n",
      "Uploading processing scripts to S3...\n",
      "  âœ“ text_analysis.py uploaded\n",
      "  âœ“ video_analysis.py uploaded\n",
      "  âœ“ aggregator.py uploaded\n",
      "\n",
      "Defining pipeline steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating pipeline...\n",
      "Pipeline 'SparkQuest-Career-Matcher-Pipeline' defined with 3 steps\n",
      "\n",
      "Deploying pipeline to SageMaker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… SUCCESS! Pipeline deployed successfully!\n",
      "============================================================\n",
      "\n",
      "Pipeline Name: SparkQuest-Career-Matcher-Pipeline\n",
      "Region: us-east-2\n",
      "Bucket: sagemaker-us-east-2-717494507037\n",
      "\n",
      "Pipeline ARN: arn:aws:sagemaker:us-east-2:717494507037:pipeline/SparkQuest-Career-Matcher-Pipeline\n",
      "\n",
      "Pipeline steps:\n",
      "  1. TextAnalysis - Analyzes personal assessment text\n",
      "  2. VideoAnalysis - Analyzes creative content video\n",
      "  3. TalentAssessmentAggregator - Matches candidate to careers\n",
      "\n",
      "============================================================\n",
      "You can now run the interactive script to execute this pipeline.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "%run sparkquest_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01af60ac-2f12-4230-976f-743b20a59182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SparkQuest Interactive Analysis ---\n",
      "\n",
      "[1/2] Please paste your personal assessment text (or transcript):\n",
      "(You can type or paste multiple lines. Press [Enter] on an empty line when you are done.)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  creative\n",
      ">  design\n",
      ">  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Assessment text captured.\n",
      "\n",
      "[2/2] Please provide the S3 URI for your creative video:\n",
      "(e.g., s3://my-videos-bucket/my-reel.mp4)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Video S3 URI:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No S3 URI provided, using default demo URI.\n",
      "NOTE: Make sure a demo video actually exists at s3://sagemaker-us-east-2-717494507037/sparkquest/demo_video.mp4\n",
      "...Video URI captured.\n",
      "\n",
      "--- Preparing and uploading inputs to S3... ---\n",
      "   -> Assessment text uploaded to: s3://sagemaker-us-east-2-717494507037/sparkquest/inputs/exec-1763235898/personal_assessment.txt\n",
      "   -> Video URI uploaded to:     s3://sagemaker-us-east-2-717494507037/sparkquest/inputs/exec-1763235898/creative_content_uri.txt\n",
      "\n",
      "--- Starting SageMaker Pipeline 'SparkQuest-Career-Matcher-Pipeline'... ---\n",
      "\n",
      "SUCCESS: Pipeline execution started.\n",
      "Execution ARN: arn:aws:sagemaker:us-east-2:717494507037:pipeline/SparkQuest-Career-Matcher-Pipeline/execution/2poe3pxjezsr\n",
      "\n",
      "---\n",
      "You can monitor the pipeline progress in the SageMaker console:\n",
      "httpsExample:://us-east-2.console.aws.amazon.com/sagemaker/home?region=us-east-2#/pipelines/executions\n",
      "\n",
      "Waiting for pipeline to complete... (This can take 5-10 minutes)\n",
      "\n",
      "--- ERROR ---\n",
      "Could not start pipeline. Did you run the 'sparkquest_pipeline.py' script first?\n",
      "Error: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# --- 1. CONFIGURE AWS & SAGEMAKER ---\n",
    "sagemaker_session = sagemaker.Session()\n",
    "boto_session = boto3.Session()\n",
    "s3_client = boto_session.client('s3')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "pipeline_name = \"SparkQuest-Career-Matcher-Pipeline\"\n",
    "\n",
    "# --- 2. DEFINE HELPER FUNCTION ---\n",
    "def upload_text_to_s3(content, s3_key):\n",
    "    \"\"\"Uploads a string as a text file to S3.\"\"\"\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=default_bucket,\n",
    "            Key=s3_key,\n",
    "            Body=content\n",
    "        )\n",
    "        return f\"s3://{default_bucket}/{s3_key}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to S3: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. GET USER INPUT ---\n",
    "print(\"--- SparkQuest Interactive Analysis ---\")\n",
    "\n",
    "# Prompt for Personal Assessment\n",
    "print(\"\\n[1/2] Please paste your personal assessment text (or transcript):\")\n",
    "# --- FIX: Changed instructions to be clear ---\n",
    "print(\"(You can type or paste multiple lines. Press [Enter] on an empty line when you are done.)\")\n",
    "assessment_lines = []\n",
    "while True:\n",
    "    # --- FIX: Added a '>' prompt to make it clear it's waiting for input ---\n",
    "    line = input(\"> \") \n",
    "    if line == \"\":\n",
    "        break\n",
    "    assessment_lines.append(line)\n",
    "assessment_text = \"\\n\".join(assessment_lines)\n",
    "\n",
    "if not assessment_text:\n",
    "    print(\"No text provided, using default demo text.\")\n",
    "    assessment_text = \"I am a creative person who enjoys analytics and structured projects.\"\n",
    "\n",
    "print(\"\\n...Assessment text captured.\")\n",
    "\n",
    "# Prompt for Video S3 URI\n",
    "print(\"\\n[2/2] Please provide the S3 URI for your creative video:\")\n",
    "print(\"(e.g., s3://my-videos-bucket/my-reel.mp4)\")\n",
    "video_s3_uri = input(\"Video S3 URI: \").strip()\n",
    "\n",
    "if not video_s3_uri:\n",
    "    print(\"No S3 URI provided, using default demo URI.\")\n",
    "    # You MUST upload a demo file to this exact path for the default to work\n",
    "    video_s3_uri = f\"s3://{default_bucket}/sparkquest/demo_video.mp4\" \n",
    "    print(f\"NOTE: Make sure a demo video actually exists at {video_s3_uri}\")\n",
    "\n",
    "print(\"...Video URI captured.\")\n",
    "\n",
    "# --- 4. PREPARE & UPLOAD INPUTS ---\n",
    "print(\"\\n--- Preparing and uploading inputs to S3... ---\")\n",
    "\n",
    "# Create a unique execution ID\n",
    "execution_id = f\"exec-{int(time.time())}\"\n",
    "\n",
    "# Define S3 paths for the text files\n",
    "assessment_s3_key = f\"sparkquest/inputs/{execution_id}/personal_assessment.txt\"\n",
    "video_uri_s3_key = f\"sparkquest/inputs/{execution_id}/creative_content_uri.txt\"\n",
    "\n",
    "# Upload the collected data as text files to S3\n",
    "assessment_s3_path = upload_text_to_s3(assessment_text, assessment_s3_key)\n",
    "# We don't upload the video, just the *path* to the video in a text file\n",
    "video_uri_s3_path = upload_text_to_s3(video_s3_uri, video_uri_s3_key)\n",
    "\n",
    "if not (assessment_s3_path and video_uri_s3_path):\n",
    "    print(\"Failed to upload input files to S3. Aborting.\")\n",
    "    # Use exit() in a script, but in a notebook, we'll just return\n",
    "    # exit() \n",
    "else:\n",
    "    print(f\"   -> Assessment text uploaded to: {assessment_s3_path}\")\n",
    "    print(f\"   -> Video URI uploaded to:     {video_uri_s3_path}\")\n",
    "\n",
    "    # --- 5. START THE PIPELINE ---\n",
    "    print(f\"\\n--- Starting SageMaker Pipeline '{pipeline_name}'... ---\")\n",
    "\n",
    "    try:\n",
    "        # Get the pipeline object by name\n",
    "        pipeline = Pipeline(\n",
    "            name=pipeline_name,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "        )\n",
    "\n",
    "        # Start the pipeline execution, passing in the S3 paths\n",
    "        # of the files we just created as parameters.\n",
    "        execution = pipeline.start(\n",
    "            parameters={\n",
    "                \"PersonalAssessmentS3Uri\": assessment_s3_path,\n",
    "                \"CreativeContentS3Uri\": video_uri_s3_path,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        execution_arn = execution.arn \n",
    "        print(f\"\\nSUCCESS: Pipeline execution started.\")\n",
    "        print(f\"Execution ARN: {execution_arn}\")\n",
    "        \n",
    "        print(\"\\n---\")\n",
    "        print(\"You can monitor the pipeline progress in the SageMaker console:\")\n",
    "        print(f\"httpsExample:://{boto_session.region_name}.console.aws.amazon.com/sagemaker/home?region={boto_session.region_name}#/pipelines/executions\")\n",
    "        \n",
    "        # Wait for the execution to finish\n",
    "        print(\"\\nWaiting for pipeline to complete... (This can take 5-10 minutes)\")\n",
    "        execution.wait()\n",
    "        \n",
    "        print(\"Pipeline execution FINISHED.\")\n",
    "        \n",
    "        # --- 6. GET AND DISPLAY RESULTS ---\n",
    "        print(\"\\n--- Retrieving Final Report... ---\")\n",
    "        \n",
    "        # Find the S3 output of the final step\n",
    "        step_list = execution.list_steps()\n",
    "        final_step_output = None\n",
    "        for step in step_list:\n",
    "            if step['StepName'] == 'TalentAssessmentAggregator':\n",
    "                # Find the output named 'final_job_list'\n",
    "                for output in step['Metadata']['ProcessingJob']['ProcessingOutputConfig']['Outputs']:\n",
    "                    if output['OutputName'] == 'final_job_list':\n",
    "                        output_s3_uri = output['S3Output']['S3Uri']\n",
    "                        final_step_output = os.path.join(output_s3_uri, 'final_job_list.json')\n",
    "                        break\n",
    "            if final_step_output:\n",
    "                break\n",
    "                \n",
    "        if final_step_output:\n",
    "            print(f\"Loading results from: {final_step_output}\")\n",
    "            \n",
    "            # Parse S3 URI\n",
    "            s3_path_parts = final_step_output.replace(\"s3://\", \"\").split(\"/\")\n",
    "            bucket = s3_path_parts[0]\n",
    "            key = \"/\".join(s3_path_parts[1:])\n",
    "            \n",
    "            # Download and print the file\n",
    "            result_obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "            result_data = json.loads(result_obj['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            print(\"\\n--- ðŸš€ Your SparkQuest Career Report ---\")\n",
    "            print(json.dumps(result_data, indent=2))\n",
    "            \n",
    "        else:\n",
    "            print(\"Could not find the final output step. Please check the pipeline execution details.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- ERROR ---\")\n",
    "        print(f\"Could not start pipeline. Did you run the 'sparkquest_pipeline.py' script first?\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b8ec1-7ad6-4481-ae8f-c4458557ccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da4985-a0e8-48ef-9f02-07fa7010be1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71585c4b-b0f7-4172-8699-c391fa3b8e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99728012-d598-404e-ac5f-60d983c682ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0310f33c-803e-4892-8b84-449ca9fb34ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07115cbb-397c-4783-a1aa-cc88176725a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167e942-d0de-4d3b-be62-3e4a3a7f05db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548e498-93e1-4e06-8f08-5446acfbac47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf46462-129d-464e-86ac-2b2ffc724ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
